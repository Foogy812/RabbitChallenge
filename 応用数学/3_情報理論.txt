■自己情報量
　人は情報量の差異を差分の大きさではなく、差分の大きさの比率でとらえているように思える。
　よって物理的な情報量をwとすると、上記の感覚的な情報量変化はΔW/Wと表せる。
　これを積分すると
　　自己情報量（感覚的情報量） = logW = -logP （P = 1/W　※Wを確率に変換）
　　　※単位対数の底が2…bit、底がe…nat

■シャノンエントロピー
　自己情報量の期待値がシャノンエントロピーであり、下式で表せる。
　　xが離散値の時　…　-ΣP(x)logP(x)
　　xが連続値の時　…　-∫P(x)logP(x)dx
　　
■KLダイバージェンス
　KLダイバージェンスは、同じ事象・確率変数における確率分布の違いを表す。
　これは、一方の分布を基準とした時の他方のシャノンエントロピーといえる。
　
■交差エントロピー
　KLダイバージェンスの一部を取り出したもの
　KLダイバージェンス同様、2つの分布が似ているほど、交差エントロピーは小さくなる。
　機械学習による分類における予測精度の指標として使われることが多い。
　予測の分布が正解の分布に似ているほど、交差エントロピーは小さくなる。
